# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m19-6ZcYvPjWLj0vRfbmncNt6LNl3sKO
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data
import time

mnist=input_data.read_data_sets("MNIST_data")

#defining generator network
def generator(z,reuse=None):
    with tf.variable_scope('gen',reuse=reuse):
        hidden1=tf.layers.dense(inputs=z,units=128,activation=tf.nn.relu)
        hidden2=tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.relu)
        output=tf.layers.dense(inputs=hidden2,units=784,activation=tf.nn.tanh)
        
        return output
    
def discriminator(X,reuse=None):
    with tf.variable_scope('dis',reuse=reuse):
        hidden1=tf.layers.dense(inputs=X,units=128,activation=tf.nn.relu)
        hidden2=tf.layers.dense(inputs=hidden1,units=128,activation=tf.nn.relu)
        logits=tf.layers.dense(hidden2,units=1)
        output=tf.sigmoid(logits)
        
        return output,logits

tf.reset_default_graph()
#real images is the placeholder of the real image from mnist data set
real_images=tf.placeholder(tf.float32,shape=[None,784])
#z is a placeholdere for random 100 size vector
z=tf.placeholder(tf.float32,shape=[None,100])

G=generator(z)
D_output_real,D_logits_real=discriminator(real_images)
D_output_fake,D_logits_fake=discriminator(G,reuse=True)

def loss_func(logits_in,labels_in):
    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))

def acc_func(logits_in, labels_in):
  return tf.reduce_mean(tf.cast(tf.equal(logits_in, labels_in), tf.float32))
  #return tf.reduce_mean(tf.metrics.accuracy(predictions=logits_in,labels=labels_in))
  #return logits_in

D_real_loss=loss_func(D_logits_real,tf.ones_like(D_logits_real)*0.9)
D_fake_loss=loss_func(D_logits_fake,tf.zeros_like(D_logits_real))
D_loss=D_real_loss+D_fake_loss



G_loss= loss_func(D_logits_fake,tf.ones_like(D_logits_fake))
#G_acc= acc_func(G,real_images)
#G_acc= acc_func(D_logits_fake,tf.ones_like(D_logits_fake))
G_acc= acc_func(G,real_images)

lr=0.005

tvars=tf.trainable_variables()
print(tvars)
d_vars=[var for var in tvars if 'dis' in var.name]
g_vars=[var for var in tvars if 'gen' in var.name]

D_trainer=tf.train.AdamOptimizer(lr).minimize(D_loss,var_list=d_vars)
G_trainer=tf.train.AdamOptimizer(lr).minimize(G_loss,var_list=g_vars)
# D_trainer=tf.train.GradientDescentOptimizer(lr).minimize(D_loss,var_list=d_vars)
# G_trainer=tf.train.GradientDescentOptimizer(lr).minimize(G_loss,var_list=g_vars)

batch_size=128
epochs=300
init=tf.global_variables_initializer()

samples=[] #generator examples
D_loss_list=[]
G_loss_list=[]
G_acc_list=[]
fig, axes = plt.subplots(nrows=1, ncols=3, figsize = (30,5))

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(epochs):
        num_batches=mnist.train.num_examples//batch_size
        for i in range(num_batches):
            batch=mnist.train.next_batch(batch_size)
            batch_images=batch[0].reshape((batch_size,784))
            #print(batch_images[0])
            batch_images=batch_images*2-1
            #print(batch_images[0])
            batch_z=np.random.normal(0,0.33,size=(batch_size,100))
#             _, D_loss_curr=sess.run([D_trainer,D_loss],feed_dict={real_images:batch_images,z:batch_z})
#             _, G_loss_curr=sess.run([G_trainer,G_loss],feed_dict={z:batch_z})
            _=sess.run(D_trainer,feed_dict={real_images:batch_images,z:batch_z})
            _=sess.run(G_trainer,feed_dict={z:batch_z})
            D_loss_curr=sess.run(D_loss,feed_dict={real_images:batch_images,z:batch_z})
            G_loss_curr=sess.run(G_loss,feed_dict={z:batch_z})
            G_acc_curr=sess.run(G_acc,feed_dict={z:batch_z, real_images:batch_images})
            D_loss_list.append(D_loss_curr)
            G_loss_list.append(G_loss_curr)
            G_acc_list.append(G_acc_curr)
        print("on epoch{}".format(epoch))
        
        sample_z=np.random.normal(0,0.33,size=(1,100))
        gen_sample=sess.run(generator(z,reuse=True),feed_dict={z:sample_z})
        
        samples.append(gen_sample)
        #print(gen_sample.reshape(28,28))
        #plt.imshow(gen_sample.reshape(28,28))
axes[0].plot(D_loss_list,label="Discriminator Loss")
axes[1].plot(G_loss_list,label="Generator Loss")
axes[2].plot(G_acc_list)
#axes[3].imshow(samples[198].reshape(28,28), cmap=plt.cm.binary)

axes[0].grid(True)
axes[0].set_xlabel("--- Epochs --->")
axes[0].set_ylabel("--- Discriminator Loss --->")
axes[0].legend()
axes[1].grid(True)
axes[1].set_xlabel("--- Epochs --->")
axes[1].set_ylabel("--- Generator Loss --->")
axes[1].legend()
axes[2].grid(True)
axes[2].set_xlabel("--- Epochs --->")
axes[2].set_ylabel("--- Generator Accuracy --->")
axes[2].legend()

i=0
while(i<=299):
  plt.imshow(samples[i].reshape(28,28), cmap=plt.cm.binary)
  plt.show()
  i+=50
